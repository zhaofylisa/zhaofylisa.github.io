<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: July 15, 2025 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.3.1" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Fengyue Zhao" />

  
  
  
    
  
  <meta name="description" content=" - Fengyue Zhao, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. Motivation Infants initially discriminate most sound contrasts but quickly attune to those of their native language. This raises the question: how do infants identify the relevant acoustic dimensions for learning phonetic categories? The distributional learning account proposes that infants track the distribution of sounds, and identify acoustic dimensions as contrastive if their distribution has two or more distinct peaks (i.e. multimodal distributions) [1]. However, while multimodality appear in controlled experiments, they are rarely found in naturalistic, highly variable speech, suggesting that multimodality is not a reliable way to identify contrastive dimensions [2]. Recent work comparing languages with/without vowel length contrasts suggests that even without multimodality, contrastive dimensions show more contextual variability: when a dimension is contrastive, the shape of its distribution will vary more across contexts [3]. The distributional learning across contexts hypothesis proposes that infants utilize this contextual variability to distinguish phonetic categories. This study tests this hypothesis by examining Hong Kong Cantonese tones, exploring whether ease of acquiring different tonal contrasts is linked to their contextual variability in distribution shape. Cantonese serves as a valuable test case due to the overlapping acoustic distributions between its six tones: high-level (T1), high-rising (T2), mid-level (T3), low-falling (T4), low-rising (T5), and low-level (T6).
" />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/project/dl/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/emerald.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  
    
    <link href="/css/custom.min.c421a923556ea37dceba59bdb1c869fb40a2561e88c4e31a125ff574a1b2a9d7.css" rel="stylesheet" />
  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu_36d38cc90d602a4.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_a7bdbb7aac91ba59.png" />

  <link rel="canonical" href="http://localhost:1313/project/dl/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Fengyue Zhao" />
  <meta property="og:url" content="http://localhost:1313/project/dl/" />
  <meta property="og:title" content="Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech | Fengyue Zhao" />
  <meta property="og:description" content=" - Fengyue Zhao, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. 
Motivation
Infants initially discriminate most sound contrasts but quickly attune to those of their native language. This raises the question: how do infants identify the relevant acoustic dimensions for learning phonetic categories? The distributional learning account proposes that infants track the distribution of sounds, and identify acoustic dimensions as contrastive if their distribution has two or more distinct peaks (i.e. multimodal distributions) [1]. However, while multimodality appear in controlled experiments, they are rarely found in naturalistic, highly variable speech, suggesting that multimodality is not a reliable way to identify contrastive dimensions [2]. Recent work comparing languages with/without vowel length contrasts suggests that even without multimodality, contrastive dimensions show more contextual variability: when a dimension is contrastive, the shape of its distribution will vary more across contexts [3]. The distributional learning across contexts hypothesis proposes that infants utilize this contextual variability to distinguish phonetic categories. This study tests this hypothesis by examining Hong Kong Cantonese tones, exploring whether ease of acquiring different tonal contrasts is linked to their contextual variability in distribution shape. Cantonese serves as a valuable test case due to the overlapping acoustic distributions between its six tones: high-level (T1), high-rising (T2), mid-level (T3), low-falling (T4), low-rising (T5), and low-level (T6)." /><meta property="og:image" content="http://localhost:1313/project/dl/featured.png" />
    <meta property="twitter:image" content="http://localhost:1313/project/dl/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2024-12-30T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2024-12-30T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/project/dl/"
  },
  "headline": "Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech",
  
  "image": [
    "http://localhost:1313/project/dl/featured.png"
  ],
  
  "datePublished": "2024-12-30T00:00:00Z",
  "dateModified": "2024-12-30T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Fengyue Zhao"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Fengyue Zhao",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/media/icon_hu_4fb1741a7e83138b.png"
    }
  },
  "description": "\u003cp\u003e\u003csup\u003e - \u003cstrong\u003eFengyue Zhao\u003c/strong\u003e, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. \u003c/sup\u003e\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eInfants initially discriminate most sound contrasts but quickly attune to those of their native language. This raises the question: how do infants identify the relevant acoustic dimensions for learning phonetic categories? The distributional learning account proposes that infants track the distribution of sounds, and identify acoustic dimensions as contrastive if their distribution has two or more distinct peaks (i.e. multimodal distributions) [1]. However, while multimodality appear in controlled experiments, they are rarely found in naturalistic, highly variable speech, suggesting that multimodality is not a reliable way to identify contrastive dimensions [2]. Recent work comparing languages with/without vowel length contrasts suggests that even without multimodality, contrastive dimensions show more contextual variability: when a dimension is contrastive, the shape of its distribution will vary more across contexts [3]. The distributional learning across contexts hypothesis proposes that infants utilize this contextual variability to distinguish phonetic categories. This study tests this hypothesis by examining Hong Kong Cantonese tones, exploring whether ease of acquiring different tonal contrasts is linked to their contextual variability in distribution shape. Cantonese serves as a valuable test case due to the overlapping acoustic distributions between its six tones: high-level (T1), high-rising (T2), mid-level (T3), low-falling (T4), low-rising (T5), and low-level (T6).\u003c/p\u003e"
}
</script>

  

  


  <title>Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech | Fengyue Zhao</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  







<link type="text/css" rel="stylesheet" href="/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css" integrity="sha256-vnZutBkxehTsdp0hbpd5v&#43;jzc3yA54D0ug2vtXpBpII=" />


<script src="/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js" integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script>


<script>window.hbb.pagefind = {"baseUrl":"/"};</script>

<style>
  html.dark {
    --pagefind-ui-primary: #eeeeee;
    --pagefind-ui-text: #eeeeee;
    --pagefind-ui-background: #152028;
    --pagefind-ui-border: #152028;
    --pagefind-ui-tag: #152028;
  }
</style>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({
      element: "#search",
      showSubResults: true,
      baseUrl: window.hbb.pagefind.baseUrl,
      bundlePath: window.hbb.pagefind.baseUrl + "pagefind/",
    });
  });
  document.addEventListener('DOMContentLoaded', () => {
    let element = document.getElementById('search');
    let trigger = document.getElementById('search_toggle');

    if (trigger) {
      trigger.addEventListener('click', () => {
        element.classList.toggle('hidden');
        element.querySelector("input").value = ""
        element.querySelector("input").focus()

        if (!element.classList.contains('hidden')) {
          let clear_trigger = document.querySelector('.pagefind-ui__search-clear');

          if (clear_trigger && !clear_trigger.hasAttribute('listenerOnClick')) {
            clear_trigger.setAttribute('listenerOnClick', 'true');

            clear_trigger.addEventListener('click', () => {
              element.classList.toggle('hidden');
            });
          }
        }

      });
    }
  });
</script>















  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Fengyue Zhao">
        Fengyue Zhao
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >About</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/projects"
        >Research</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/teaching"
        >Teaching</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/name"
        >My name</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      
      <button
        aria-label="search"
        class="text-black hover:text-primary  inline-block px-3 text-xl dark:text-white"
        id="search_toggle">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512" fill="currentColor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352a144 144 0 1 0 0-288 144 144 0 1 0 0 288z"/></svg>
      </button>
      

      
      

      
      

      
      
    </div>
  </nav>
</header>


<div id="search" class="hidden p-3"></div>


        
      
    </div>
    <div class="page-body  my-10">
      





<div class="mx-auto flex max-w-screen-xl">
  



<aside class="hb-sidebar-container max-lg:[transform:translate3d(0,-100%,0)] lg:hidden xl:block">
  
  <div class="px-4 pt-4 lg:hidden">
    
    
  </div>
  <div class="hb-scrollbar lg:h-[calc(100vh-var(--navbar-height))]">
    <ul class="flex flex-col gap-1 lg:hidden">
      
      
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/"
    
  >News
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/2025interspeech/"
    
  >🗣️ I’m presenting at Interspeech 2025 in Rotterdam!
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/2025bsmc/"
    
  >🗣️ I gave a podium talk at the Boston Speech Motor Control Symposium!
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/2025tai/"
    
  >🗣️ I gave a talk at TAI 2025!
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/2025a-exam/"
    
  >🎉 I passed my second qualifying exam and became a phd candidate!
    </a>
              
            </li></ul>
      </div></li>
        <li class="open"><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/"
    
  >Projects
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/tfp/"
    
  >Unpredictable Temporal Auditory Feedback Perturbation Induces Lengthening, Not Compensation
    </a>
              
            </li><li class="flex flex-col open"><a
    class="hb-sidebar-custom-link
      sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900"
    href="/project/dl/"
    
  >Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech
    </a>
  
    <ul class="hb-sidebar-mobile-toc"><li>
              <a
                href="#motivation"
                class="hb-docs-link"
              >Motivation</a>
            </li>
          <li>
              <a
                href="#method"
                class="hb-docs-link"
              >Method</a>
            </li>
          <li>
              <a
                href="#results"
                class="hb-docs-link"
              >Results</a>
            </li>
          <li>
              <a
                href="#discussions"
                class="hb-docs-link"
              >Discussions</a>
            </li>
          <li>
              <a
                href="#references"
                class="hb-docs-link"
              >References</a>
            </li>
          </ul>
  
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/nn/"
    
  >Syllable Position Prominence in Unsupervised Neural Network Segment Categorization
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/rsa/"
    
  >Probabilistic Listener: A Case of Reflexive ziji &#34;self&#34; Ambiguity Resolution in Mandarin
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/teaching/"
    
  >Teaching
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/projects/"
    
  >Projects
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/name/"
    
  >My name
    </a></li>
    </ul>

    <div class="max-xl:hidden h-0 w-64 shrink-0"></div></div>

</aside>
  

<nav class="hb-toc order-last hidden w-64 shrink-0 xl:block print:hidden px-4" aria-label="table of contents">
  











  <div class="hb-scrollbar text-sm [hyphens:auto] sticky top-16 overflow-y-auto pr-4 pt-6 max-h-[calc(100vh-var(--navbar-height)-env(safe-area-inset-bottom))] -mr-4 rtl:-ml-4"><p class="mb-4 font-semibold tracking-tight">On this page</p><ul>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#motivation">Motivation</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#method">Method</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#results">Results</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#discussions">Discussions</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#references">References</a>
      </li></ul>

  
  
    
    
  



    












  </div>
  </nav>


  <article class="w-full break-words flex min-h-[calc(100vh-var(--navbar-height))] min-w-0 justify-center pb-8 pr-[calc(env(safe-area-inset-right)-1.5rem)]">
    <main class="w-full min-w-0 max-w-6xl px-6 pt-4 md:px-12">

      

      <h1 class="mt-2 text-4xl font-bold tracking-tight text-slate-900 dark:text-slate-100">Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech</h1>

      <div class="mt-4 mb-16">
      <div class="text-gray-500 dark:text-gray-300 text-sm flex items-center flex-wrap gap-y-2"><span class="mr-1">Dec 30, 2024</span><span class="mx-1">·</span>
        
          
          
          <div class="group inline-flex items-center text-current gap-x-1.5 mx-1">
            
            
            <img src="/author/fengyue-zhao/avatar_hu_8754bcc3e98e329f.webp" alt="Fengyue Zhao" class="inline-block h-4 w-4 rounded-full border border-current" loading="lazy" />
            
            <div >Fengyue Zhao</div>
          </div>
          
        

        
        <span class="mx-1">·</span>
        <span class="mx-1">
          5 min read
        </span>
        
        </div>

        <div class="mt-3">
          




<div class="">
  
  








  
    
  



<a class="hb-attachment-link hb-attachment-large" href="/uploads/DL/DL.pdf" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>

















</div>


        </div>
      </div>



      
      
      
      
      

      
      
      
      
      
      
      
      
      
      
      <div class="article-header article-container featured-image-wrapper mt-4 mb-16" style="max-width: 720px; max-height: 566px;">
        <div style="position: relative">
          <img src="/project/dl/featured_hu_a6947273fb7efeb8.webp" width="720" height="566" alt="" class="featured-image">
          
        </div>
      </div>
      

      
      

      <div class="prose prose-slate lg:prose-xl dark:prose-invert">
        <p><sup> - <strong>Fengyue Zhao</strong>, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. </sup></p>
<h2 id="motivation">Motivation</h2>
<p>Infants initially discriminate most sound contrasts but quickly attune to those of their native language. This raises the question: how do infants identify the relevant acoustic dimensions for learning phonetic categories? The distributional learning account proposes that infants track the distribution of sounds, and identify acoustic dimensions as contrastive if their distribution has two or more distinct peaks (i.e. multimodal distributions) [1]. However, while multimodality appear in controlled experiments, they are rarely found in naturalistic, highly variable speech, suggesting that multimodality is not a reliable way to identify contrastive dimensions [2]. Recent work comparing languages with/without vowel length contrasts suggests that even without multimodality, contrastive dimensions show more contextual variability: when a dimension is contrastive, the shape of its distribution will vary more across contexts [3]. The distributional learning across contexts hypothesis proposes that infants utilize this contextual variability to distinguish phonetic categories. This study tests this hypothesis by examining Hong Kong Cantonese tones, exploring whether ease of acquiring different tonal contrasts is linked to their contextual variability in distribution shape. Cantonese serves as a valuable test case due to the overlapping acoustic distributions between its six tones: high-level (T1), high-rising (T2), mid-level (T3), low-falling (T4), low-rising (T5), and low-level (T6).</p>
<h2 id="method">Method</h2>
<p>We analyzed the Multi-ethnic Hong Kong Cantonese Corpus (MeHKCC) [4], which consists naturalistic speech recordings from 24 native Cantonese female speakers. 65,106 monosyllabic and disyllabic content words were extracted. Pairwise F0 contour comparisons showed varying acoustic overlap among tones, except for the phonetically distinct T1 (e.g., distinct pair: T1T4, overlapping pairs: T3T5, T2T5; see Fig.1). Based on acoustic overlap and documented acquisition difficulty [5], tone pairs were categorized into: (1) Easy pairs, which are phonetically distinct and easy to learn (e.g., T1T4); (2) Hard pairs, which are acoustically overlapping but learnable (e.g., T3T5); and (3) Merger pairs, which are acoustically overlapping and challenging to learn (e.g., T2T5). We predict that contextual variability in distribution shapes aligns with developmental acquisition patterns, with Easy contrasts showing the most separation and variability, followed by Hard and Merger pairs. Although Hard and Merger pairs both show acoustic overlap, we predict that Hard pairs are more learnable due to the greater contextual variability in their distributional shapes.</p>
<p>To test this, nine F0 landmarks (mean, median, variance, max-min, onset, 25%, 75%, offset, duration) were extracted, and t-distributed stochastic neighbor embedding (t-SNE) was used to reduce these dimensions to a 2D space. Distributional differences were quantified using Earth Mover’s Distance (EMD) for pairwise tone comparisons across contexts. Contexts were defined as combinations of (1) neighboring sounds (e.g., stops, fricatives, nasals), (2) syllable position in a word (i.e., first or second syllable in a word), and (3) prosodic position (i.e., utterance-initial, -medial or -final).</p>
<figure>
  <img src="/uploads/DL/fig1.png"/>
  <figcaption>Figure 1. Pairwise F0 contours for three example tone pairs from a female speaker, with mean and 95% confidence intervals, time-normalized. (A) T1T4 shows clear phonetic distinction, while (B) T3T5 and (C) T2T5 exhibit varying degrees of overlap.</figcaption>
</figure>
<h2 id="results">Results</h2>
<p>Analyses were conducted for all tone pairs, with T1T4 (Easy), T3T5 (Hard), and T2T5 (Merger) selected for illustration. Fig. 2 shows the frequency distribution of the Hard tone pair T3T5 after dimensionality reduction. While tone pairs show unimodal distribution when pooled across contexts (Panel A), they show different distribution shapes across specific contexts (Panel B shows two illustrative contexts). Figure 3 presents a boxplot of EMD for the three tone pairs, where each data point represents the pairwise EMD of two tones within a single context. Higher mean EMD values indicate greater distributional separation in general, while higher variance across contexts reflects greater contextual variability. Across four EMD metrics—mean, median, variance, and maximum—Easy pairs consistently show the highest values, followed by Hard pairs, and then Merger pairs (values provided in Fig. 3). This hierarchy aligns with developmental acquisition patterns: tones with greater separation and contextual variability are learned more readily than tones with lower values. Analyses of all 15 tone pairs reveal similar trends, with more nuanced interactions between distributional learning across contexts and acoustic realizations.</p>
<figure>
  <img src="/uploads/DL/fig2.png"/>
  <figcaption>Figure 2. (A) The overall frequency distribution of the Hard tone pair T3T5 along a reduced 2D space shows a unimodal distribution, despite T3 and T5 being contrastive tones. (B) Frequency distributions for the same tone pair across two specific contexts reveal different distributional shapes. Context 2 (e.g., nasal onsets without codas, second syllable, utterance-medial) exhibits greater separability (EMD = 78.3) compared to Context 1 (EMD = 12.2). Even though the overall frequency distribution is unimodal, we can see differently shaped distributions across context.
  </figcaption>
</figure>
<figure>
  <img src="/uploads/DL/fig3.png"/>
  <figcaption>Figure 3. Earth Mover’s Distance (EMD) distributions for three tone pair categories: Easy (T1T4), Hard (T3T5), and Merger (T2T5). Each data point represents the pairwise EMD between two tones within a specific context. Panel A illustrates a context with the greatest separability, while Panel B shows a context with the lowest separability. Lowest values for four EMD metrics (mean, median, variance, and maximum) were bolded.
  </figcaption>
</figure>
<h2 id="discussions">Discussions</h2>
<p>This study explored the learning of multiple tone contrasts, a relatively unexplored area in distributional learning. Findings suggest that infants may rely on distributional shapes across contexts to learn contrasts, offering a plausible mechanism for learning in the absence of invariance in speech signals. Future direction will expand to additional corpora with additional contexts, and develop computational learning models to quantitatively capture the learning trajectories of all tone pairs.</p>
<h2 id="references">References</h2>
<p>[1] Maye, J., Werker, J. F., &amp; Gerken, L. (2002). Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82(3), B101-111. <a href="https://doi.org/10.1016/s0010-0277%2801%2900157-3" target="_blank" rel="noopener">https://doi.org/10.1016/s0010-0277(01)00157-3</a></p>
<p>[2] Bion, R. A. H., Miyazawa, K., Kikuchi, H., &amp; Mazuka, R. (2013). Learning Phonemic Vowel Length from Naturalistic Recordings of Japanese Infant-Directed Speech. PLOS ONE, 8(2), e51594. <a href="https://doi.org/10.1371/journal.pone.0051594" target="_blank" rel="noopener">https://doi.org/10.1371/journal.pone.0051594</a></p>
<p>[3] Hitczenko, K., &amp; Feldman, N. (2022). Naturalistic speech supports distributional learning across contexts. <a href="https://doi.org/10.1073/pnas.2123230119" target="_blank" rel="noopener">https://doi.org/10.1073/pnas.2123230119</a></p>
<p>[4] Yu, A., Delisle, N., Martin, N., Zhang, V., Yao, Y., &amp; To, C. (2024). The Multi-ethnic Hong Kong Cantonese Corpus. CorpusPhon satellite workshop at LabPhon19. <a href="https://ccds.edu.hku.hk" target="_blank" rel="noopener">https://ccds.edu.hku.hk</a></p>
<p>[5] Mok, P. P. K., Fung, H. S. H., &amp; Li, V. G. (2019). Assessing the Link Between Perception and Production in Cantonese Tone Acquisition. Journal of Speech, Language, and Hearing Research, 62(5), 1243–1257. <a href="https://doi.org/10.1044/2018_JSLHR-S-17-0430" target="_blank" rel="noopener">https://doi.org/10.1044/2018_JSLHR-S-17-0430</a></p>

      </div>

      
  <time class="mt-12 mb-8 block text-xs text-gray-500 ltr:text-right rtl:text-left dark:text-gray-400" datetime="2024-12-30T00:00:00.000Z">
    <span>Last updated on</span>
    Dec 30, 2024</time>

      <div class="container mx-auto prose prose-slate lg:prose-xl dark:prose-invert mt-5">
        
        <div class="max-w-prose print:hidden">
  
  

  

<div class="flex justify-center">
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/distributional-learning/">Distributional Learning</a>
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/tones/">Tones</a>
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/computational-modeling/">Computational Modeling</a>
  
</div>


  

  








  
  
    



  
  
  
    
  
  
  

<div class="flex pt-12 pb-4">
  
  
  <img
    class="mr-4 h-24 w-24 rounded-full"
    width="96"
    height="96"
    alt="Fengyue Zhao"
  src="/author/fengyue-zhao/avatar_hu_4fb1741a7e83138b.png"
  loading="lazy"
  />
  
  <div class="place-self-center">
    <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
      Authors
    </div>
    <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
      <a href="http://localhost:1313/" class="no-underline">
      Fengyue Zhao
      </a>
    </div>

    
    <div class="text-sm font-bold text-neutral-700 dark:text-neutral-300">
    PhD Candidate in linguistics
    </div>
    


    

    <div class="text-2xl sm:text-lg pt-1">

      
<div class="flex flex-wrap text-neutral-500 dark:text-neutral-300">
  
    
    
    
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="mailto:fz227@cornell.edu"
      
      aria-label="At-Symbol"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5 0 1 1-9 0a4.5 4.5 0 0 1 9 0Zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 1 0-2.636 6.364M16.5 12V8.25"/></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://github.com/zhaofylisa"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Brands/Github"
    ><svg style="height: 1em;" fill="currentColor" viewBox="3 3 18 18"><path d="M12 3C7.0275 3 3 7.12937 3 12.2276C3 16.3109 5.57625 19.7597 9.15374 20.9824C9.60374 21.0631 9.77249 20.7863 9.77249 20.5441C9.77249 20.3249 9.76125 19.5982 9.76125 18.8254C7.5 19.2522 6.915 18.2602 6.735 17.7412C6.63375 17.4759 6.19499 16.6569 5.8125 16.4378C5.4975 16.2647 5.0475 15.838 5.80124 15.8264C6.51 15.8149 7.01625 16.4954 7.18499 16.7723C7.99499 18.1679 9.28875 17.7758 9.80625 17.5335C9.885 16.9337 10.1212 16.53 10.38 16.2993C8.3775 16.0687 6.285 15.2728 6.285 11.7432C6.285 10.7397 6.63375 9.9092 7.20749 9.26326C7.1175 9.03257 6.8025 8.08674 7.2975 6.81794C7.2975 6.81794 8.05125 6.57571 9.77249 7.76377C10.4925 7.55615 11.2575 7.45234 12.0225 7.45234C12.7875 7.45234 13.5525 7.55615 14.2725 7.76377C15.9937 6.56418 16.7475 6.81794 16.7475 6.81794C17.2424 8.08674 16.9275 9.03257 16.8375 9.26326C17.4113 9.9092 17.76 10.7281 17.76 11.7432C17.76 15.2843 15.6563 16.0687 13.6537 16.2993C13.98 16.5877 14.2613 17.1414 14.2613 18.0065C14.2613 19.2407 14.25 20.2326 14.25 20.5441C14.25 20.7863 14.4188 21.0746 14.8688 20.9824C16.6554 20.364 18.2079 19.1866 19.3078 17.6162C20.4077 16.0457 20.9995 14.1611 21 12.2276C21 7.12937 16.9725 3 12 3Z"></path></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://orcid.org/0000-0002-4357-8342"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Academicons/Orcid"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M336.62 194.538c-7.13-3.328-13.866-5.56-20.253-6.614c-6.365-1.095-16.574-1.612-30.71-1.612h-36.704v152.747h37.634c14.673 0 26.081-1.013 34.224-3.017c8.142-2.004 14.921-4.526 20.356-7.626a69.448 69.448 0 0 0 14.942-11.388c14.488-14.714 21.742-33.273 21.742-55.717c0-22.052-7.44-40.052-22.341-53.982c-5.498-5.166-11.822-9.444-18.89-12.793zM256 8C119.022 8 8 119.042 8 256s111.022 248 248 248s248-111.042 248-248S392.978 8 256 8Zm-82.336 357.513h-29.389V160.148h29.389zM158.95 138.696c-11.14 0-20.213-9.01-20.213-20.212c0-11.118 9.052-20.191 20.213-20.191c11.18 0 20.232 9.052 20.232 20.191a20.194 20.194 0 0 1-20.232 20.212zm241.386 163.597c-5.29 12.545-12.834 23.581-22.65 33.088c-9.982 9.837-21.597 17.194-34.844 22.196c-7.75 3.017-14.839 5.063-21.307 6.117c-6.49 1.013-18.828 1.509-37.076 1.509h-64.956V160.148h69.233c27.962 0 50.034 4.154 66.32 12.545c16.265 8.37 29.181 20.728 38.792 36.972c9.61 16.265 14.425 34.018 14.425 53.196c.023 13.765-2.666 26.908-7.936 39.432z"/></svg></a>
  
</div>



    </div>
  </div>
</div>



  




  
  
    
    
    
      
      
    
<div class="pt-1 no-prose w-full">
  <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
  <div class="flex flex-col md:flex-row flex-nowrap justify-between gap-5 pt-2">
    <div class="">
      
        <a class="group flex no-underline" href="/project/tfp/">
          <span
            class="mt-[-0.3rem] me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
          ><span class="ltr:inline rtl:hidden">&larr;</span></span>
          <span class="flex flex-col">
            <span
              class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
            >Unpredictable Temporal Auditory Feedback Perturbation Induces Lengthening, Not Compensation</span>
            <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
              
                Mar 29, 2025
              
            </span>
          </span>
        </a>
      
    </div>
    <div class="">
      
        <a class="group flex text-right no-underline" href="/project/nn/">
          <span class="flex flex-col">
            <span
              class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
            >Syllable Position Prominence in Unsupervised Neural Network Segment Categorization</span
            >
            <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
              
                Mar 30, 2024
              
            </span>
          </span>
          <span
            class="mt-[-0.3rem] ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
          ><span class="ltr:inline">&rarr;</span></span>
        </a>
      
    </div>
  </div>
</div>



  


  



</div>

      </div>

    </main>
  </article>
</div>

    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  <p class="powered-by text-center">
    © 2025 Fengyue Zhao.
  </p>
  





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
