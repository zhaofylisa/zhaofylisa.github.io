<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Fengyue Zhao</title>
    <link>http://localhost:1313/project/</link>
      <atom:link href="http://localhost:1313/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 30 Dec 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_c71e624ff7a064be.png</url>
      <title>Projects</title>
      <link>http://localhost:1313/project/</link>
    </image>
    
    <item>
      <title>Syllable Position Prominence in Unsupervised Neural Network Segment Categorization</title>
      <link>http://localhost:1313/project/nn/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/nn/</guid>
      <description>&lt;p&gt;&lt;sup&gt; - &lt;strong&gt;Fengyue Zhao&lt;/strong&gt;, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. &lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;English obstruents exhibit diverse phonetic realizations across syllable positions, like /t/ and /p/ in words such as top and pot [1]. Linguistically we assume that phone identity—(e.g. /p/ vs. /t/) is a strong predictor of representational similarity, while syllable position—e.g. onset vs. coda—is perhaps a secondary factor. But is this always the case? Unsupervised learning in neural networks presents a practical approach for exploring this interplay, because it does not require presuppositions about phonological categories such as segments and syllable. Previous studies [2, 3] have demonstrated the capacity of neural networks to learn abstract representations from acoustic signals. This study employed an unsupervised autoencoder neural network to explore the correlation between phonological categories and network-learned representations. Surprisingly, we found that for consonants, syllable position plays a larger role in representational similarity than phone identity.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;&lt;u&gt; Data&lt;/u&gt;: In order to enhance the interpretability of neural models, we chose to use a controlled experimental dataset to investigate the interplay between syllable position and segment identity. The dataset comprised nine syllables corresponding to all combinations of {p, t, Ø}onset × /a/ ×{p, t, Ø}coda (note that onset Ø was realized as glottal stop [ʔ]), with /p/ and /t/ appearing in onset position (denoted as p1 and t1) and coda position (p2 and t2). Examples included [p1at2], [ʔap2], etc. The syllables were articulated following an initial prolonged [i]. &lt;u&gt;Model&lt;/u&gt;: The autoencoder architecture [2] (see Fig. 1a) encompassed an encoder compressing acoustic input into a latent representation (labeled R) and a decoder reconstructing the input. The inputs were 12-coefficient MFCC vectors (window: 25ms hamming, hop time: 1ms, range: 0 - 8000 Hz). Notably, the autoencoder acquired a compact representation of the input in the latent representation R, without utilizing explicit labeling during training. The data were divided into training (60%), validation (20%), and test sets (20%). &lt;u&gt;Analysis&lt;/u&gt;: During testing, all 7 individual categories (i.e., aa, iy, ʔ, p1, t1, p2, t2) were fed into the autoencoder model. Subsequently, extracted latent representations (patterns of node activation in R) were subjected to dimensionality reduction via t-SNE (t-distributed Stochastic Neighbor Embedding), followed by K-means clustering to assess activation pattern similarities within the R space  (Fig. 1b).&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://localhost:1313/uploads/NN/method.png&#34;/&gt;
  &lt;figcaption&gt;Figure 1. (a) Model: an autoencoder architecture. (b) Analysis: the analysis prosedure.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;For consonants, syllable position emerged as a stronger predictor of representational similarity than segment identity. In Fig. 2a, consonants sharing the same syllable position but different identities (e.g. onset p1, t1, ʔ) were closer to each other in the representation space compared to consonants of the same identity in different positions (e.g. onset p1 and coda p2). This observation was further supported by the k-means clustering result. Fig. 2a shows the locations of individual segment tokens in representational space, along with grey convex hulls surrounding clusters. With k=4 clusters, k-means algorithm effectively separated the two vowels [i] and [a], while also forming clusters of onset sounds (p1, t1, and ʔ) and coda sounds (p2, t2). Certain instances of glottal stops [ʔ] were grouped with [i]’s, potentially explained by varied phonetic realizations — some as full glottal plosives while others may exhibit creakiness. Sub-clusters for [i] and [a] were associated with individual speakers. The k=4 clustering achieved strong performance metrics (V-Measure: 0.78, Completeness: 0.90, Homogeneity: 0.69) compared to other values of k. Fig. 2b shows that increasing the cluster count does not lead to distinct clusters associated with segment identity. The V-measure score, a harmonic mean between completeness and homogeneity, was in fact maximal for k=4, suggesting that separating onsets and codas provides the optimal clustering of sound categories.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://localhost:1313/uploads/NN/results2.png&#34;/&gt;
  &lt;figcaption&gt; &lt;p align=&#39;left&#39;&gt;Figure 2. (a) K-means clustering result when k=4. This illustration shows the locations of individual segment tokens in representational space R, along with grey convex hulls surrounding clusters. The boxed annotations summarize the 4 clusters: [i]’s, [a]’s, onsets, and codas. (b) Evaluation metrics (V-Measure, Completeness, Homogeneity) of k-means clustering across k values (2 to 10), illustrating performance fluctuations with cluster count. &lt;/p&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Our results show that syllable position is more influential than segment identity in representational space learned by the neural network from acoustic signals. This is important because it suggests that the role of syllable position in human representations may be underappreciated. Future exploration of this finding with larger datasets will be of value.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Turk, A. (1994). Phonological Structure and Phonetic Form: Articulatory phonetic clues to syllable affiliation: gestural characteristics of bilabial stops.&lt;/p&gt;
&lt;p&gt;[2] Shain, C., &amp;amp; Elsner, M. (2019). Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 69–85.&lt;/p&gt;
&lt;p&gt;[3] Shain, C., &amp;amp; Elsner, M. (2020). Acquiring language from speech by learning to remember and predict. Proceedings of the 24th Conference on Computational Natural Language Learning, 195–214.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic Listener: A Case of Reflexive ziji &#34;self&#34; Ambiguity Resolution in Mandarin</title>
      <link>http://localhost:1313/project/rsa/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/rsa/</guid>
      <description>&lt;p&gt;&lt;sup&gt; (This work was my honors thesis at UMass Amherst, and was advised by &lt;a href = &#34;https://www.umass.edu/linguistics/member/brian-dillon&#34;&gt; Dr. Brian Dillon &lt;/a&gt; and &lt;a href = &#34;https://linguistics.uchicago.edu/people/ming-xiang&#34;&gt; Dr. Ming Xiang &lt;/a&gt;.) &lt;/sup&gt;
&lt;br&gt;
&lt;sup&gt; - &lt;strong&gt;Fengyue Zhao&lt;/strong&gt;, Brian Dillon, Ming Xiang. Probabilistic Listener: A Case of Chinese Mandarin Reflexive ziji. Ambiguity Resolution. 36th Annual Conference on Human Sentence Processing. March 9 - 11 2023. University of Pittsburgh, Pittsburgh, PA. &lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;What mechanisms do people use to resolve ambiguous pronouns? Prior studies have tried to identify factors that contribute to the resolution mechanisms, such as first-mention biases [1], verb semantics [2], and world knowledge[3]. These principles usually only make categorical predictions instead of continuous predictions about experimental data. Probabilistic models are another promising attempt [4][5][6][7]. This study aims to test the generality of two Bayesian models — the Simple Bayesian model (SBM) and the Rational Speech Act model (RSA) — on pronoun ambiguity resolution in Mandarin Chinese in the case of the reflexive pronoun ziji &amp;ldquo;self&amp;rdquo;. SBM has been examined to make good quantitative predictions for Chinese, but only for across-sentence relations in discourses with personal pronouns [7]. RSA has been examined in pronoun resolution in French and English [6]. This study provides cross-linguistic evidence to support both Bayesian models, investigating within-sentence relations with Chinese reflexive pronouns which are often considered to be regulated by a distinct set of grammatical principles than discourse anaphors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  (1)  [小红]i  说    [小明]j   总      把      [自己]i/j    弄糊涂。
      [Hong]i  says  [Ming]j always   BA     [ziji]i/j   confuses.
      ‘[Hong]i says that [Ming]j always confuses [self]i/j.’
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-resolution-of-reflexive-ziji&#34;&gt;The resolution of reflexive ziji&lt;/h2&gt;
&lt;p&gt;Reflexive in Mandarin may take a simplex form — ziji “self” which permits both local (e.g. Ming in 1) and non-local (e.g. Hong in 1) referent interpretation [8]. In Experiment 1 (Nsubj=135, Nitem=30), we measured Mandarin speakers’ preferences for resolving ziji in ambiguous sentences with an antecedent selection task. Stimuli have the sentence structure in (1), differing only in the verb. We found that comprehenders preferred local antecedents 59% of the time on average. This non-randomly distributed result showed that people have strategies to resolve this ambiguous reflexive. Moreover, the preference of non-local antecedents ranged between 9% and 88% across items. This non-uniform result showed that preferences and extent of preferences differ across stimuli items. Therefore, item-by-item quantitative analysis is necessary to test if two models can capture this item variation.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://localhost:1313/uploads/RSA/RSA_pic1.png&#34; alt=&#34;Formulars&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;two-bayesian-models&#34;&gt;Two Bayesian Models&lt;/h2&gt;
&lt;p&gt;The Simple Bayesian model (SBM) models the listener’s probability of selecting  a specific referent as proportional to their prior that this referent will be mentioned next, and the likelihood that a speaker will produce this pronoun when signaling a specific referent (see Formula 1) [5]. The Rational Speech Act model (RSA) suggests that listeners assume speakers are rational agents who have already chosen the best utterance among all possible options to convey intended information. Listeners combine this recursive thinking with their prior world knowledge to interpret ambiguous pronouns (see Formula 2) [4]. To evaluate these models’ fit against our data, we estimated: P(referent) in both models in Experiment 2, a world knowledge bias test (Nsubj=28, Nitem=30); P(utterance | referent) in SBM in Experiment 3, a pronoun production task (Nsubj=65, Nitem=30); Cost(utterance) in RSA in a corpus study (Ntoken=16.5 billion) which is the logarithm of the frequency of each pronoun in a certain sentence structure. If listeners processing ambiguous pronouns follows SBM/RSA, there should be a strong correlation between the Experiment 1 results and SBM/RSA predictions.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;http://localhost:1313/uploads/RSA/RSA_pic2.png&#34; alt=&#34;Trulli&#34; style=&#34;width:50%&#34;&gt;
&lt;img src=&#34;http://localhost:1313/uploads/RSA/RSA_pic3.png&#34; alt=&#34;Trulli&#34; style=&#34;width:50%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.1 - Fit RSA model predictions (top) and SBM model predictions (bottom) with experimental data from Experiment 1 on an item-by-item basis.&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;R-squared&lt;/th&gt;
          &lt;th&gt;MSE&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;MAE&lt;/th&gt;
          &lt;th&gt;P-value&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;SBM&lt;/td&gt;
          &lt;td&gt;0.598&lt;/td&gt;
          &lt;td&gt;0.025&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.138&lt;/td&gt;
          &lt;td&gt;0.0007&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;RSA&lt;/td&gt;
          &lt;td&gt;0.674&lt;/td&gt;
          &lt;td&gt;0.006&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.048&lt;/td&gt;
          &lt;td&gt;0.0002&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p style =&#34;text-align: center;&#34;&gt;&lt;b&gt;&lt;span style=&#34;font-size:0.8em&#34;&gt;Table 1. R-squared, MSE, MAE, and P-value of the two models for model evaluations.&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;h2 id=&#34;results-and-discussions&#34;&gt;Results and Discussions&lt;/h2&gt;
&lt;p&gt;Item-by-item quantitative analysis in Fig.1 shows that both SBM and RSA can make accurate predictions for the resolution of the ambiguous pronoun ziji (R2&amp;gt;0.59, p&amp;lt;0.001), providing a case study to support that within-sentence reflexive binding obeys principles of Bayesian inference too. Meanwhile, although both SBM and RSA are Bayesian models, the statistical results in Table 1 show that RSA performs a little better while SBM overestimates the non-local antecedent choices and underestimates the local antecedent choices. The difference between two models is that listeners in SBM reason about the production of the pronoun directly using their own experience without encoding explicit the frequency of pronouns, while listeners in RSA are rational and reason indirectly by thinking about how a rational speaker would choose pronouns, and this rational speaker would explicitly take the frequency of pronouns into account (the Cost term in Formula 2). One possible explanation for this is that a multilevel recursive reasoning between listeners and speakers coupled with the explicitly encoded frequency of pronouns could enhance model predictions, leading to a better fit between RSA and the experimental data.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Järvikivi, J., van Gompel, R. P., Hyönä, J., &amp;amp; Bertram, R. (2005). Psychological Science.&lt;/p&gt;
&lt;p&gt;[2] Koornneef A. W., &amp;amp; van Berkum J. J. A. (2006). Journal of Memory and Language.&lt;/p&gt;
&lt;p&gt;[3] Hobbs, J. R. (1979). Cognitive Science.&lt;/p&gt;
&lt;p&gt;[4] Frank, M. C., &amp;amp; Goodman, N. D. (2012). Science.&lt;/p&gt;
&lt;p&gt;[5] Kehler, A, &amp;amp; Rohde, H. (2013). Theoretical Linguistics.&lt;/p&gt;
&lt;p&gt;[6] Schulz, M., Burnett, H., &amp;amp; Hemforth, B. (2021). A Journal of General Linguistics.&lt;/p&gt;
&lt;p&gt;[7] Zhan, M., Levy, R., Kehler, A. (2020). PLOS ONE.&lt;/p&gt;
&lt;p&gt;[8] Huang, C.-T. J., Li, Y.-h. A., &amp;amp; Li, Y. (2009). Cambridge University Press.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
