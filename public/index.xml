<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fengyue Zhao</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Fengyue Zhao</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu_88080d99dd003e3b.png</url>
      <title>Fengyue Zhao</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>üó£Ô∏è I‚Äôm presenting at Interspeech 2025 in Rotterdam!</title>
      <link>https://example.com/post/2025interspeech/</link>
      <pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2025interspeech/</guid>
      <description>&lt;p&gt;I‚Äôm excited to present my paper &amp;ldquo;The Role of Contextual Variation in Learning Cantonese Tones from Naturalistic Speech&amp;rdquo;, co-authored with Dr. Jennifer Kuo, at the 26th edition of the Interspeech Conference (Interspeech 2025), held in Rotterdam, The Netherlands from August 17‚Äì21.&lt;/p&gt;
&lt;p&gt;I look forward to sharing our findings and connecting with fellow researchers in speech science!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üó£Ô∏è I gave a podium talk at the Boston Speech Motor Control Symposium!</title>
      <link>https://example.com/post/2025bsmc/</link>
      <pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2025bsmc/</guid>
      <description>&lt;p&gt;In June 2025, I presented my research titled &amp;ldquo;Unpredictable Temporal Auditory Feedback Perturbation Induces Lengthening, Not Compensation&amp;rdquo;, co-authored with Dr. Sam Tilsen, at the 5th Biennial Boston Speech Motor Control Symposium (BSMCS‚Äô25), hosted at Boston University.&lt;/p&gt;
&lt;p&gt;I was grateful to receive a Student Travel Award from the Boston Speech Motor Control Symposium to support my participation in this event.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üéâ I passed my second qualifying exam and became a phd candidate!</title>
      <link>https://example.com/post/2025a-exam/</link>
      <pubDate>Fri, 09 May 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2025a-exam/</guid>
      <description>&lt;p&gt;In May 2025, I successfully passed my second qualifying exam and officially advanced to PhD candidacy in Linguistics at Cornell University.&lt;/p&gt;
&lt;p&gt;My dissertation is titled &lt;em&gt;&amp;ldquo;The Role of Contextual Variation in Learning Cantonese Tones from Naturalistic Speech.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;My committee consists of Professor Jennifer Kuo (Co-Chair), Professor Marten van Schijndel (Co-Chair), and Professor Samuel Tilsen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üó£Ô∏è I gave a talk at TAI 2025!</title>
      <link>https://example.com/post/2025tai/</link>
      <pubDate>Fri, 09 May 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2025tai/</guid>
      <description>&lt;p&gt;In May 2025, I presented my talk &amp;ldquo;Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech&amp;rdquo;, co-authored with Dr. Jennifer Kuo, at the 3rd International Conference on Tone and Intonation (TAI 2025), held at the Institute for Phonetics and Speech Processing, Ludwig-Maximilians-Universit√§t in Munich, Germany.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m thankful to the International Phonetic Association for supporting my participation with an IPA Student Award, which helped make this international travel possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unpredictable Temporal Auditory Feedback Perturbation Induces Lengthening, Not Compensation</title>
      <link>https://example.com/project/tfp/</link>
      <pubDate>Sat, 29 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/tfp/</guid>
      <description>&lt;!-- 
&lt;sup&gt; - &lt;strong&gt;Fengyue Zhao&lt;/strong&gt;, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. &lt;/sup&gt; --&gt;
&lt;p&gt;Temporal feedback perturbation (TFP) studies show that speakers can learn compensatory responses when the timing of the perturbation is predictable, and that phonological factors such as stress and segment identity can modulate the response. This study investigates whether similar response patterns occur when the timing of TFP relative to an utterance cannot be anticipated, which we refer to as &lt;strong&gt;unpredictable temporal feedback perturbation (UTFP).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We applied subtle TFPs at random times within a phrase ‚ÄúI saw papa yesterday‚Äù, where ‚Äúpapa‚Äù was pronounced with either a strong-weak or weak-strong stress pattern. Seventeen native English speakers produced 280 utterances each, with perturbations occurring in 80% of trials. Each perturbation lasted 198 ms, with the first half slowed (rate = 0.56) and the second half sped up (rate = 5), consistent with previous studies. Crucially, unlike standard TFP, the timing of the perturbation was distributed randomly and approximately uniformly across the entire utterance, rather than targeting a specific segmental event.&lt;/p&gt;
&lt;p&gt;Results revealed a response profile distinct from standard predictable TFP: (a) responses consistently resulted in lengthening, rather than shortening; (b) responses effects were largest 500 ms after perturbation onset, substantially later than those of compensatory responses to predictable TFP. Phonological modulation patterns resembled those in standard TFP experiments, with vowels exhibiting larger effects than consonants and larger effects in stressed syllables than unstressed ones.&lt;/p&gt;
&lt;p&gt;These findings suggest that the speech motor control feedback system responds to TFP differently when perturbations are unpredictable, and yet is influenced by similar phonological factors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>https://example.com/teaching/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://example.com/teaching/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://example.com/uploads/syllabus/cl1.pdf&#34; target=&#34;_blank&#34;&gt;Computational Linguistics 1&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Spring 2025, Cornell University.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://example.com/uploads/syllabus/chin.pdf&#34; target=&#34;_blank&#34;&gt;Intermediate Chinese Reading and Writing for Students of Chinese Heritage&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Fall 2024, Cornell University.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://example.com/uploads/syllabus/ph2.pdf&#34; target=&#34;_blank&#34;&gt;Introduction to Phonetics and Phonology&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Spring 2024, Cornell University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://example.com/uploads/syllabus/cogsci.pdf&#34; target=&#34;_blank&#34;&gt;Introduction to Cognitive Science&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Fall 2023, Cornell University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Calculus for Life and Social Sciences II&lt;/strong&gt;&lt;br&gt;
Fall 2021, Spring 2022ÔºåUniversity of Massachusetts Amherst&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Elementary Chinese&lt;/strong&gt;&lt;br&gt;
Fall 2018ÔºåUniversity of Massachusetts Amherst&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Distributional Learning Across Contexts: Learning Cantonese Tones in Naturalistic Speech</title>
      <link>https://example.com/project/dl/</link>
      <pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/dl/</guid>
      <description>&lt;p&gt;&lt;sup&gt; - &lt;strong&gt;Fengyue Zhao&lt;/strong&gt;, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. &lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Infants initially discriminate most sound contrasts but quickly attune to those of their native language. This raises the question: how do infants identify the relevant acoustic dimensions for learning phonetic categories? The distributional learning account proposes that infants track the distribution of sounds, and identify acoustic dimensions as contrastive if their distribution has two or more distinct peaks (i.e. multimodal distributions) [1]. However, while multimodality appear in controlled experiments, they are rarely found in naturalistic, highly variable speech, suggesting that multimodality is not a reliable way to identify contrastive dimensions [2]. Recent work comparing languages with/without vowel length contrasts suggests that even without multimodality, contrastive dimensions show more contextual variability: when a dimension is contrastive, the shape of its distribution will vary more across contexts [3]. The distributional learning across contexts hypothesis proposes that infants utilize this contextual variability to distinguish phonetic categories. This study tests this hypothesis by examining Hong Kong Cantonese tones, exploring whether ease of acquiring different tonal contrasts is linked to their contextual variability in distribution shape. Cantonese serves as a valuable test case due to the overlapping acoustic distributions between its six tones: high-level (T1), high-rising (T2), mid-level (T3), low-falling (T4), low-rising (T5), and low-level (T6).&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;We analyzed the Multi-ethnic Hong Kong Cantonese Corpus (MeHKCC) [4], which consists naturalistic speech recordings from 24 native Cantonese female speakers. 65,106 monosyllabic and disyllabic content words were extracted. Pairwise F0 contour comparisons showed varying acoustic overlap among tones, except for the phonetically distinct T1 (e.g., distinct pair: T1T4, overlapping pairs: T3T5, T2T5; see Fig.1). Based on acoustic overlap and documented acquisition difficulty [5], tone pairs were categorized into: (1) Easy pairs, which are phonetically distinct and easy to learn (e.g., T1T4); (2) Hard pairs, which are acoustically overlapping but learnable (e.g., T3T5); and (3) Merger pairs, which are acoustically overlapping and challenging to learn (e.g., T2T5). We predict that contextual variability in distribution shapes aligns with developmental acquisition patterns, with Easy contrasts showing the most separation and variability, followed by Hard and Merger pairs. Although Hard and Merger pairs both show acoustic overlap, we predict that Hard pairs are more learnable due to the greater contextual variability in their distributional shapes.&lt;/p&gt;
&lt;p&gt;To test this, nine F0 landmarks (mean, median, variance, max-min, onset, 25%, 75%, offset, duration) were extracted, and t-distributed stochastic neighbor embedding (t-SNE) was used to reduce these dimensions to a 2D space. Distributional differences were quantified using Earth Mover‚Äôs Distance (EMD) for pairwise tone comparisons across contexts. Contexts were defined as combinations of (1) neighboring sounds (e.g., stops, fricatives, nasals), (2) syllable position in a word (i.e., first or second syllable in a word), and (3) prosodic position (i.e., utterance-initial, -medial or -final).&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://example.com/uploads/DL/fig1.png&#34;/&gt;
  &lt;figcaption&gt;Figure 1. Pairwise F0 contours for three example tone pairs from a female speaker, with mean and 95% confidence intervals, time-normalized. (A) T1T4 shows clear phonetic distinction, while (B) T3T5 and (C) T2T5 exhibit varying degrees of overlap.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Analyses were conducted for all tone pairs, with T1T4 (Easy), T3T5 (Hard), and T2T5 (Merger) selected for illustration. Fig. 2 shows the frequency distribution of the Hard tone pair T3T5 after dimensionality reduction. While tone pairs show unimodal distribution when pooled across contexts (Panel A), they show different distribution shapes across specific contexts (Panel B shows two illustrative contexts). Figure 3 presents a boxplot of EMD for the three tone pairs, where each data point represents the pairwise EMD of two tones within a single context. Higher mean EMD values indicate greater distributional separation in general, while higher variance across contexts reflects greater contextual variability. Across four EMD metrics‚Äîmean, median, variance, and maximum‚ÄîEasy pairs consistently show the highest values, followed by Hard pairs, and then Merger pairs (values provided in Fig. 3). This hierarchy aligns with developmental acquisition patterns: tones with greater separation and contextual variability are learned more readily than tones with lower values. Analyses of all 15 tone pairs reveal similar trends, with more nuanced interactions between distributional learning across contexts and acoustic realizations.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://example.com/uploads/DL/fig2.png&#34;/&gt;
  &lt;figcaption&gt;Figure 2. (A) The overall frequency distribution of the Hard tone pair T3T5 along a reduced 2D space shows a unimodal distribution, despite T3 and T5 being contrastive tones. (B) Frequency distributions for the same tone pair across two specific contexts reveal different distributional shapes. Context 2 (e.g., nasal onsets without codas, second syllable, utterance-medial) exhibits greater separability (EMD = 78.3) compared to Context 1 (EMD = 12.2). Even though the overall frequency distribution is unimodal, we can see differently shaped distributions across context.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://example.com/uploads/DL/fig3.png&#34;/&gt;
  &lt;figcaption&gt;Figure 3. Earth Mover‚Äôs Distance (EMD) distributions for three tone pair categories: Easy (T1T4), Hard (T3T5), and Merger (T2T5). Each data point represents the pairwise EMD between two tones within a specific context. Panel A illustrates a context with the greatest separability, while Panel B shows a context with the lowest separability. Lowest values for four EMD metrics (mean, median, variance, and maximum) were bolded.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;discussions&#34;&gt;Discussions&lt;/h2&gt;
&lt;p&gt;This study explored the learning of multiple tone contrasts, a relatively unexplored area in distributional learning. Findings suggest that infants may rely on distributional shapes across contexts to learn contrasts, offering a plausible mechanism for learning in the absence of invariance in speech signals. Future direction will expand to additional corpora with additional contexts, and develop computational learning models to quantitatively capture the learning trajectories of all tone pairs.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Maye, J., Werker, J. F., &amp;amp; Gerken, L. (2002). Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82(3), B101-111. &lt;a href=&#34;https://doi.org/10.1016/s0010-0277%2801%2900157-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/s0010-0277(01)00157-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Bion, R. A. H., Miyazawa, K., Kikuchi, H., &amp;amp; Mazuka, R. (2013). Learning Phonemic Vowel Length from Naturalistic Recordings of Japanese Infant-Directed Speech. PLOS ONE, 8(2), e51594. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0051594&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1371/journal.pone.0051594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Hitczenko, K., &amp;amp; Feldman, N. (2022). Naturalistic speech supports distributional learning across contexts. &lt;a href=&#34;https://doi.org/10.1073/pnas.2123230119&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1073/pnas.2123230119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Yu, A., Delisle, N., Martin, N., Zhang, V., Yao, Y., &amp;amp; To, C. (2024). The Multi-ethnic Hong Kong Cantonese Corpus. CorpusPhon satellite workshop at LabPhon19. &lt;a href=&#34;https://ccds.edu.hku.hk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ccds.edu.hku.hk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Mok, P. P. K., Fung, H. S. H., &amp;amp; Li, V. G. (2019). Assessing the Link Between Perception and Production in Cantonese Tone Acquisition. Journal of Speech, Language, and Hearing Research, 62(5), 1243‚Äì1257. &lt;a href=&#34;https://doi.org/10.1044/2018_JSLHR-S-17-0430&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1044/2018_JSLHR-S-17-0430&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://example.com/projects/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>My name</title>
      <link>https://example.com/name/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/name/</guid>
      <description>&lt;!-- &lt;strong&gt;About my name:&lt;/strong&gt; --&gt;
&lt;p&gt;&amp;ldquo;Fengyue ‰∏∞ÊÇ¶&amp;rdquo; [f ä≈ã‚Åµ‚Åµ.…•…õ‚Åµ¬π] is my Chinese name. &amp;ldquo;Lisa&amp;rdquo; is also my name. Both names are fine for me.
My Chinese name is pronounced as:  &lt;br&gt;&lt;br&gt;
&lt;audio src=&#34;https://example.com/uploads/name.wav&#34; controls&gt;&lt;/audio&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Syllable Position Prominence in Unsupervised Neural Network Segment Categorization</title>
      <link>https://example.com/project/nn/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/nn/</guid>
      <description>&lt;p&gt;&lt;sup&gt; - &lt;strong&gt;Fengyue Zhao&lt;/strong&gt;, Sam Tilsen. Syllable Position Prominence in Unsupervised Neural Network Segment Categorization. LabPhon 19. June 27 - 29 2024. Hanyang University, Seoul, South Korea. &lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;English obstruents exhibit diverse phonetic realizations across syllable positions, like /t/ and /p/ in words such as top and pot [1]. Linguistically we assume that phone identity‚Äî(e.g. /p/ vs. /t/) is a strong predictor of representational similarity, while syllable position‚Äîe.g. onset vs. coda‚Äîis perhaps a secondary factor. But is this always the case? Unsupervised learning in neural networks presents a practical approach for exploring this interplay, because it does not require presuppositions about phonological categories such as segments and syllable. Previous studies [2, 3] have demonstrated the capacity of neural networks to learn abstract representations from acoustic signals. This study employed an unsupervised autoencoder neural network to explore the correlation between phonological categories and network-learned representations. Surprisingly, we found that for consonants, syllable position plays a larger role in representational similarity than phone identity.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;&lt;u&gt; Data&lt;/u&gt;: In order to enhance the interpretability of neural models, we chose to use a controlled experimental dataset to investigate the interplay between syllable position and segment identity. The dataset comprised nine syllables corresponding to all combinations of {p, t, √ò}onset √ó /a/ √ó{p, t, √ò}coda (note that onset √ò was realized as glottal stop [ î]), with /p/ and /t/ appearing in onset position (denoted as p1 and t1) and coda position (p2 and t2). Examples included [p1at2], [ îap2], etc. The syllables were articulated following an initial prolonged [i]. &lt;u&gt;Model&lt;/u&gt;: The autoencoder architecture [2] (see Fig. 1a) encompassed an encoder compressing acoustic input into a latent representation (labeled R) and a decoder reconstructing the input. The inputs were 12-coefficient MFCC vectors (window: 25ms hamming, hop time: 1ms, range: 0 - 8000 Hz). Notably, the autoencoder acquired a compact representation of the input in the latent representation R, without utilizing explicit labeling during training. The data were divided into training (60%), validation (20%), and test sets (20%). &lt;u&gt;Analysis&lt;/u&gt;: During testing, all 7 individual categories (i.e., aa, iy,  î, p1, t1, p2, t2) were fed into the autoencoder model. Subsequently, extracted latent representations (patterns of node activation in R) were subjected to dimensionality reduction via t-SNE (t-distributed Stochastic Neighbor Embedding), followed by K-means clustering to assess activation pattern similarities within the R space  (Fig. 1b).&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://example.com/uploads/NN/method.png&#34;/&gt;
  &lt;figcaption&gt;Figure 1. (a) Model: an autoencoder architecture. (b) Analysis: the analysis prosedure.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;For consonants, syllable position emerged as a stronger predictor of representational similarity than segment identity. In Fig. 2a, consonants sharing the same syllable position but different identities (e.g. onset p1, t1,  î) were closer to each other in the representation space compared to consonants of the same identity in different positions (e.g. onset p1 and coda p2). This observation was further supported by the k-means clustering result. Fig. 2a shows the locations of individual segment tokens in representational space, along with grey convex hulls surrounding clusters. With k=4 clusters, k-means algorithm effectively separated the two vowels [i] and [a], while also forming clusters of onset sounds (p1, t1, and  î) and coda sounds (p2, t2). Certain instances of glottal stops [ î] were grouped with [i]‚Äôs, potentially explained by varied phonetic realizations ‚Äî some as full glottal plosives while others may exhibit creakiness. Sub-clusters for [i] and [a] were associated with individual speakers. The k=4 clustering achieved strong performance metrics (V-Measure: 0.78, Completeness: 0.90, Homogeneity: 0.69) compared to other values of k. Fig. 2b shows that increasing the cluster count does not lead to distinct clusters associated with segment identity. The V-measure score, a harmonic mean between completeness and homogeneity, was in fact maximal for k=4, suggesting that separating onsets and codas provides the optimal clustering of sound categories.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://example.com/uploads/NN/results2.png&#34;/&gt;
  &lt;figcaption&gt; &lt;p align=&#39;left&#39;&gt;Figure 2. (a) K-means clustering result when k=4. This illustration shows the locations of individual segment tokens in representational space R, along with grey convex hulls surrounding clusters. The boxed annotations summarize the 4 clusters: [i]‚Äôs, [a]‚Äôs, onsets, and codas. (b) Evaluation metrics (V-Measure, Completeness, Homogeneity) of k-means clustering across k values (2 to 10), illustrating performance fluctuations with cluster count. &lt;/p&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Our results show that syllable position is more influential than segment identity in representational space learned by the neural network from acoustic signals. This is important because it suggests that the role of syllable position in human representations may be underappreciated. Future exploration of this finding with larger datasets will be of value.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Turk, A. (1994). Phonological Structure and Phonetic Form: Articulatory phonetic clues to syllable affiliation: gestural characteristics of bilabial stops.&lt;/p&gt;
&lt;p&gt;[2] Shain, C., &amp;amp; Elsner, M. (2019). Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 69‚Äì85.&lt;/p&gt;
&lt;p&gt;[3] Shain, C., &amp;amp; Elsner, M. (2020). Acquiring language from speech by learning to remember and predict. Proceedings of the 24th Conference on Computational Natural Language Learning, 195‚Äì214.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic Listener: A Case of Reflexive ziji &#34;self&#34; Ambiguity Resolution in Mandarin</title>
      <link>https://example.com/project/rsa/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/rsa/</guid>
      <description>&lt;p&gt;&lt;sup&gt; (This work was my honors thesis at UMass Amherst, and was advised by &lt;a href = &#34;https://www.umass.edu/linguistics/member/brian-dillon&#34;&gt; Dr. Brian Dillon &lt;/a&gt; and &lt;a href = &#34;https://linguistics.uchicago.edu/people/ming-xiang&#34;&gt; Dr. Ming Xiang &lt;/a&gt;.) &lt;/sup&gt;
&lt;br&gt;
&lt;sup&gt; - &lt;strong&gt;Fengyue Zhao&lt;/strong&gt;, Brian Dillon, Ming Xiang. Probabilistic Listener: A Case of Chinese Mandarin Reflexive ziji. Ambiguity Resolution. 36th Annual Conference on Human Sentence Processing. March 9 - 11 2023. University of Pittsburgh, Pittsburgh, PA. &lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;What mechanisms do people use to resolve ambiguous pronouns? Prior studies have tried to identify factors that contribute to the resolution mechanisms, such as first-mention biases [1], verb semantics [2], and world knowledge[3]. These principles usually only make categorical predictions instead of continuous predictions about experimental data. Probabilistic models are another promising attempt [4][5][6][7]. This study aims to test the generality of two Bayesian models ‚Äî the Simple Bayesian model (SBM) and the Rational Speech Act model (RSA) ‚Äî on pronoun ambiguity resolution in Mandarin Chinese in the case of the reflexive pronoun ziji &amp;ldquo;self&amp;rdquo;. SBM has been examined to make good quantitative predictions for Chinese, but only for across-sentence relations in discourses with personal pronouns [7]. RSA has been examined in pronoun resolution in French and English [6]. This study provides cross-linguistic evidence to support both Bayesian models, investigating within-sentence relations with Chinese reflexive pronouns which are often considered to be regulated by a distinct set of grammatical principles than discourse anaphors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  (1)  [Â∞èÁ∫¢]i  ËØ¥    [Â∞èÊòé]j   ÊÄª      Êää      [Ëá™Â∑±]i/j    ÂºÑÁ≥äÊ∂Ç„ÄÇ
      [Hong]i  says  [Ming]j always   BA     [ziji]i/j   confuses.
      ‚Äò[Hong]i says that [Ming]j always confuses [self]i/j.‚Äô
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-resolution-of-reflexive-ziji&#34;&gt;The resolution of reflexive ziji&lt;/h2&gt;
&lt;p&gt;Reflexive in Mandarin may take a simplex form ‚Äî ziji ‚Äúself‚Äù which permits both local (e.g. Ming in 1) and non-local (e.g. Hong in 1) referent interpretation [8]. In Experiment 1 (Nsubj=135, Nitem=30), we measured Mandarin speakers‚Äô preferences for resolving ziji in ambiguous sentences with an antecedent selection task. Stimuli have the sentence structure in (1), differing only in the verb. We found that comprehenders preferred local antecedents 59% of the time on average. This non-randomly distributed result showed that people have strategies to resolve this ambiguous reflexive. Moreover, the preference of non-local antecedents ranged between 9% and 88% across items. This non-uniform result showed that preferences and extent of preferences differ across stimuli items. Therefore, item-by-item quantitative analysis is necessary to test if two models can capture this item variation.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/uploads/RSA/RSA_pic1.png&#34; alt=&#34;Formulars&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;two-bayesian-models&#34;&gt;Two Bayesian Models&lt;/h2&gt;
&lt;p&gt;The Simple Bayesian model (SBM) models the listener‚Äôs probability of selecting  a specific referent as proportional to their prior that this referent will be mentioned next, and the likelihood that a speaker will produce this pronoun when signaling a specific referent (see Formula 1) [5]. The Rational Speech Act model (RSA) suggests that listeners assume speakers are rational agents who have already chosen the best utterance among all possible options to convey intended information. Listeners combine this recursive thinking with their prior world knowledge to interpret ambiguous pronouns (see Formula 2) [4]. To evaluate these models‚Äô fit against our data, we estimated: P(referent) in both models in Experiment 2, a world knowledge bias test (Nsubj=28, Nitem=30); P(utterance | referent) in SBM in Experiment 3, a pronoun production task (Nsubj=65, Nitem=30); Cost(utterance) in RSA in a corpus study (Ntoken=16.5 billion) which is the logarithm of the frequency of each pronoun in a certain sentence structure. If listeners processing ambiguous pronouns follows SBM/RSA, there should be a strong correlation between the Experiment 1 results and SBM/RSA predictions.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://example.com/uploads/RSA/RSA_pic2.png&#34; alt=&#34;Trulli&#34; style=&#34;width:50%&#34;&gt;
&lt;img src=&#34;https://example.com/uploads/RSA/RSA_pic3.png&#34; alt=&#34;Trulli&#34; style=&#34;width:50%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.1 - Fit RSA model predictions (top) and SBM model predictions (bottom) with experimental data from Experiment 1 on an item-by-item basis.&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;R-squared&lt;/th&gt;
          &lt;th&gt;MSE&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;MAE&lt;/th&gt;
          &lt;th&gt;P-value&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;SBM&lt;/td&gt;
          &lt;td&gt;0.598&lt;/td&gt;
          &lt;td&gt;0.025&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.138&lt;/td&gt;
          &lt;td&gt;0.0007&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;RSA&lt;/td&gt;
          &lt;td&gt;0.674&lt;/td&gt;
          &lt;td&gt;0.006&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.048&lt;/td&gt;
          &lt;td&gt;0.0002&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p style =&#34;text-align: center;&#34;&gt;&lt;b&gt;&lt;span style=&#34;font-size:0.8em&#34;&gt;Table 1. R-squared, MSE, MAE, and P-value of the two models for model evaluations.&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;h2 id=&#34;results-and-discussions&#34;&gt;Results and Discussions&lt;/h2&gt;
&lt;p&gt;Item-by-item quantitative analysis in Fig.1 shows that both SBM and RSA can make accurate predictions for the resolution of the ambiguous pronoun ziji (R2&amp;gt;0.59, p&amp;lt;0.001), providing a case study to support that within-sentence reflexive binding obeys principles of Bayesian inference too. Meanwhile, although both SBM and RSA are Bayesian models, the statistical results in Table 1 show that RSA performs a little better while SBM overestimates the non-local antecedent choices and underestimates the local antecedent choices. The difference between two models is that listeners in SBM reason about the production of the pronoun directly using their own experience without encoding explicit the frequency of pronouns, while listeners in RSA are rational and reason indirectly by thinking about how a rational speaker would choose pronouns, and this rational speaker would explicitly take the frequency of pronouns into account (the Cost term in Formula 2). One possible explanation for this is that a multilevel recursive reasoning between listeners and speakers coupled with the explicitly encoded frequency of pronouns could enhance model predictions, leading to a better fit between RSA and the experimental data.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] J√§rvikivi, J., van Gompel, R. P., Hy√∂n√§, J., &amp;amp; Bertram, R. (2005). Psychological Science.&lt;/p&gt;
&lt;p&gt;[2] Koornneef A. W., &amp;amp; van Berkum J. J. A. (2006). Journal of Memory and Language.&lt;/p&gt;
&lt;p&gt;[3] Hobbs, J. R. (1979). Cognitive Science.&lt;/p&gt;
&lt;p&gt;[4] Frank, M. C., &amp;amp; Goodman, N. D. (2012). Science.&lt;/p&gt;
&lt;p&gt;[5] Kehler, A, &amp;amp; Rohde, H. (2013). Theoretical Linguistics.&lt;/p&gt;
&lt;p&gt;[6] Schulz, M., Burnett, H., &amp;amp; Hemforth, B. (2021). A Journal of General Linguistics.&lt;/p&gt;
&lt;p&gt;[7] Zhan, M., Levy, R., Kehler, A. (2020). PLOS ONE.&lt;/p&gt;
&lt;p&gt;[8] Huang, C.-T. J., Li, Y.-h. A., &amp;amp; Li, Y. (2009). Cambridge University Press.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
